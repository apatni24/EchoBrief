[![CI](https://github.com/apatni24/EchoBrief/actions/workflows/ci.yml/badge.svg)](https://github.com/apatni24/EchoBrief/actions/workflows/ci.yml)

# ğŸ§ EchoBrief â€” Scalable Podcast Summarization Backend

**EchoBrief** is an event-driven backend system that transcribes and summarizes podcast episodes in real time. Users simply paste a podcast episode URL, choose a summary format (bullet points, narrative, or takeaways), and receive a high-quality summary within a minute for episodes up to 30 minutes.

> ğŸ”— **Live App**: [https://echobrief.onrender.com](https://echobrief.onrender.com)  
> â±ï¸ **Summarizes a 30-minute podcast in <90 seconds**  
> ğŸŒ Deployed on low-latency infrastructure (Singapore â€“ Render + Upstash)

---

## ğŸ§  Why This Project Stands Out

EchoBrief was built to solve real-world backend engineering challenges. It showcases:

- ğŸ”„ Loose coupling across services using Redis Streams
- ğŸ“¡ Real-time updates with WebSockets for instant feedback
- ğŸ§  Scalable summarization via LLaMA 3.3 70B LLMs
- ğŸ³ Unified Dockerized deployment with internal NGINX routing
- ğŸš¦ Thoughtful design to handle cold starts and API rate limits

---

### ğŸ³ Why Only One Dockerfile in a Microservices Setup?

Although EchoBrief follows a microservices architecture, all services are hosted on a single server and exposed on different ports. An NGINX reverse proxy is used to route traffic appropriately.

This design choice avoids multiple Dockerfiles and leverages Renderâ€™s free-tier constraints efficiently.

---

## ğŸ§© Architecture

<img src="./assets/architecture.svg" alt="EchoBrief Architecture" width="500"/>

---

## ğŸ” Cold Start Optimization

Render auto-suspends inactive services, which can cause a ~25â€“30s delay on the first request. To combat this:

> âš¡ As soon as the frontend loads, it pings the backend's `/health` endpoint to **pre-warm** the server â€” ensuring fast, seamless user experience even after idle periods.

---

## ğŸ“¨ Example Payload

Hereâ€™s a sample event payload passed to the Redis Stream (`audio_uploaded`) when a podcast is fetched:

```json
{
  "file_path": "",
  "metadata": {
    "summary": "",
    "show_title": "",
    "show_summary": ""
  },
  "summary_type": "",
  "job_id": ""
}
```
**Fields explained:**

- `file_path`: Local/remote audio path  
- `metadata`: Podcast title, episode summary, and show description  
- `summary_type`: Format (e.g., `ts` = story-style/takeaways)  
- `job_id`: Correlation ID for async WebSocket updates  

---

## ğŸ› ï¸ Tech Stack

| Layer              | Tool / Service                           |
|--------------------|------------------------------------------|
| Audio Resolution   | Python + feedparser                      |
| Transcription      | AssemblyAI (diarization + speech-to-text)|
| Summarization      | LLaMA 3.3 70B Versatile (via API)        |
| Events             | Redis Streams (Upstash â€“ Singapore)      |
| Real-Time Updates  | WebSockets                               |
| Infrastructure     | Docker + NGINX + Render                  |
| Frontend           | Simple React App                         |

---

## ğŸ“ Project Structure

```plaintext
echobrief/
â”œâ”€â”€ podcast_audio_resolver_service/    # RSS parsing, file download & event emit
â”œâ”€â”€ transcription_service/             # Diarization + transcription via AssemblyAI
â”œâ”€â”€ summarization_service/             # LLM-based summarization
â”œâ”€â”€ redis_stream_client.py             # Redis Streams abstraction
â”œâ”€â”€ nginx.conf                         # Reverse proxy config (internal routing)
â”œâ”€â”€ Dockerfile                         # Container for all services
â”œâ”€â”€ start.sh                           # Entrypoint to run everything
â””â”€â”€ audio_files/                       # (Optional) Local audio storage
```

---

## ğŸ“„ Summary Types Supported

EchoBrief supports the following summary formats:

- Bullet point breakdowns
- Story-style narrative summaries
- Key actionable takeaways

---

## ğŸ“‰ API Rate Limiting

To comply with **ChatGroq API's per-minute token limit**, EchoBrief enforces:

> â³ **A 60-second cooldown between successive LLM summarization requests**

This ensures:

- No unexpected rate-limit errors during high load
- More predictable performance under concurrent usage
- Graceful queuing of summaries without affecting user experience

> The rate-limiting is implemented with an internal wait logic before calling the LLM API, keeping throughput compliant with usage constraints.

---


## ğŸš€ Getting Started (Local Dev)

```bash
# Clone the repo
git clone https://github.com/apatni24/EchoBrief.git
cd EchoBrief

# Build and run
docker build -t echobrief .
./start.sh
```

Youâ€™ll have:

- All backend microservices up on internal ports
- NGINX reverse proxy managing internal routes
- Redis Streams wiring event flow between services

---

## ğŸ–¼ï¸ Frontend Screenshots

![EchoBrief Frontend - Input Screen](./assets/frontend_input.png)  
*User inputs podcast URL and selects summary type.*

![EchoBrief Frontend - Summary Output](./assets/frontend_output.png)  
*Generated summary is displayed live via WebSocket.*

---

## ğŸ‘¨â€ğŸ’» Author

Built by [Atishay Patni](https://www.linkedin.com/in/atishaypatni)  
SDE @ Wells Fargo | Backend Engineer 

---

## ğŸ“Œ Future Enhancements

- GPU-based transcription (WhisperX + PyAnnote)
- Support for >30 min podcasts (chunking + parallel summary stitching)
- Authentication and saved summary history
- Podcast directory browsing from within the app

---
